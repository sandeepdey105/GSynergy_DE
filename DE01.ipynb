{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4011147c-d845-4268-9eb4-1f0d23cf9a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs ls /mnt/dev/Energy_DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902f537d-83c4-4c29-8764-5594e84c1cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct,regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5e11ec-838c-4307-939c-ba7801128d4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read all the tables"
    }
   },
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "file_paths = [\n",
    "    \"dbfs:/mnt/dev/Energy_DE/fact_averagecosts_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/fact_transactions_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_clnd_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_hldy_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_invloc_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_invstatus_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_possite_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_pricestate_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_prod_dlm.gz\",\n",
    "    \"dbfs:/mnt/dev/Energy_DE/hier_rtlloc_dlm.gz\"\n",
    "]\n",
    "\n",
    "# Function to load a gzipped file\n",
    "def load_gzipped_file(path):\n",
    "    return spark.read.option(\"header\", \"true\").option(\"delimiter\", \"|\").csv(path)\n",
    "\n",
    "# Load each file into a DataFrame\n",
    "fact_averagecosts_df = load_gzipped_file(file_paths[0])\n",
    "fact_transactions_df = load_gzipped_file(file_paths[1])\n",
    "hier_clnd_df = load_gzipped_file(file_paths[2])\n",
    "hier_hldy_df = load_gzipped_file(file_paths[3])\n",
    "hier_invloc_df = load_gzipped_file(file_paths[4])\n",
    "hier_invstatus_df = load_gzipped_file(file_paths[5])\n",
    "hier_possite_df = load_gzipped_file(file_paths[6])\n",
    "hier_pricestate_df = load_gzipped_file(file_paths[7])\n",
    "hier_prod_df = load_gzipped_file(file_paths[8])\n",
    "hier_rtlloc_df = load_gzipped_file(file_paths[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15c5328-684a-4d2e-86b8-4380c1f761da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- fscldt_id: string (nullable = true)\n |-- sku_id: string (nullable = true)\n |-- average_unit_standardcost: string (nullable = true)\n |-- average_unit_landedcost: string (nullable = true)\n\nroot\n |-- order_id: string (nullable = true)\n |-- line_id: string (nullable = true)\n |-- type: string (nullable = true)\n |-- dt: string (nullable = true)\n |-- pos_site_id: string (nullable = true)\n |-- sku_id: string (nullable = true)\n |-- fscldt_id: string (nullable = true)\n |-- price_substate_id: string (nullable = true)\n |-- sales_units: string (nullable = true)\n |-- sales_dollars: string (nullable = true)\n |-- discount_dollars: string (nullable = true)\n |-- original_order_id: string (nullable = true)\n |-- original_line_id: string (nullable = true)\n\nroot\n |-- fscldt_id: string (nullable = true)\n |-- fscldt_label: string (nullable = true)\n |-- fsclwk_id: string (nullable = true)\n |-- fsclwk_label: string (nullable = true)\n |-- fsclmth_id: string (nullable = true)\n |-- fsclmth_label: string (nullable = true)\n |-- fsclqrtr_id: string (nullable = true)\n |-- fsclqrtr_label: string (nullable = true)\n |-- fsclyr_id: string (nullable = true)\n |-- fsclyr_label: string (nullable = true)\n |-- ssn_id: string (nullable = true)\n |-- ssn_label: string (nullable = true)\n |-- ly_fscldt_id: string (nullable = true)\n |-- lly_fscldt_id: string (nullable = true)\n |-- fscldow: string (nullable = true)\n |-- fscldom: string (nullable = true)\n |-- fscldoq: string (nullable = true)\n |-- fscldoy: string (nullable = true)\n |-- fsclwoy: string (nullable = true)\n |-- fsclmoy: string (nullable = true)\n |-- fsclqoy: string (nullable = true)\n |-- date: string (nullable = true)\n\nroot\n |-- hldy_id: string (nullable = true)\n |-- hldy_label: string (nullable = true)\n\nroot\n |-- loc: string (nullable = true)\n |-- loc_label: string (nullable = true)\n |-- loctype: string (nullable = true)\n |-- loctype_label: string (nullable = true)\n\nroot\n |-- code_id: string (nullable = true)\n |-- code_label: string (nullable = true)\n |-- bckt_id: string (nullable = true)\n |-- bckt_label: string (nullable = true)\n |-- ownrshp_id: string (nullable = true)\n |-- ownrshp_label: string (nullable = true)\n\nroot\n |-- site_id: string (nullable = true)\n |-- site_label: string (nullable = true)\n |-- subchnl_id: string (nullable = true)\n |-- subchnl_label: string (nullable = true)\n |-- chnl_id: string (nullable = true)\n |-- chnl_label: string (nullable = true)\n\nroot\n |-- substate_id: string (nullable = true)\n |-- substate_label: string (nullable = true)\n |-- state_id: string (nullable = true)\n |-- state_label: string (nullable = true)\n\nroot\n |-- sku_id: string (nullable = true)\n |-- sku_label: string (nullable = true)\n |-- stylclr_id: string (nullable = true)\n |-- stylclr_label: string (nullable = true)\n |-- styl_id: string (nullable = true)\n |-- styl_label: string (nullable = true)\n |-- subcat_id: string (nullable = true)\n |-- subcat_label: string (nullable = true)\n |-- cat_id: string (nullable = true)\n |-- cat_label: string (nullable = true)\n |-- dept_id: string (nullable = true)\n |-- dept_label: string (nullable = true)\n |-- issvc: string (nullable = true)\n |-- isasmbly: string (nullable = true)\n |-- isnfs: string (nullable = true)\n\nroot\n |-- str: string (nullable = true)\n |-- str_label: string (nullable = true)\n |-- dstr: string (nullable = true)\n |-- dstr_label: string (nullable = true)\n |-- rgn: string (nullable = true)\n |-- rgn_label: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "fact_averagecosts_df.printSchema()\n",
    "fact_transactions_df.printSchema()\n",
    "hier_clnd_df.printSchema()\n",
    "hier_hldy_df.printSchema()\n",
    "hier_invloc_df.printSchema()\n",
    "hier_invstatus_df.printSchema()\n",
    "hier_possite_df.printSchema()\n",
    "hier_pricestate_df.printSchema()\n",
    "hier_prod_df.printSchema()\n",
    "hier_rtlloc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a91b8a-5fb2-4330-bddc-dd3f004a6b5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checking the Primary key"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in 'sku_id' column of fact_averagecosts_df: 0\nIs 'sku_id' unique in fact_averagecosts_df? False\nNull values in 'order_id' column of fact_transactions_df: 0\nIs 'order_id' unique in fact_transactions_df? False\nNull values in 'fscldt_id' column of hier_clnd_df: 0\nIs 'fscldt_id' unique in hier_clnd_df? True\nNull values in 'hldy_id' column of hier_hldy_df: 0\nIs 'hldy_id' unique in hier_hldy_df? True\nNull values in 'loc' column of hier_invloc_df: 0\nIs 'loc' unique in hier_invloc_df? True\nNull values in 'code_id' column of hier_invstatus_df: 0\nIs 'code_id' unique in hier_invstatus_df? True\nNull values in 'site_id' column of hier_possite_df: 0\nIs 'site_id' unique in hier_possite_df? True\nNull values in 'substate_id' column of hier_pricestate_df: 0\nIs 'substate_id' unique in hier_pricestate_df? True\nNull values in 'sku_id' column of hier_prod_df: 0\nIs 'sku_id' unique in hier_prod_df? True\nNull values in 'str' column of hier_rtlloc_df: 0\nIs 'str' unique in hier_rtlloc_df? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataframes = {\n",
    "    \"fact_averagecosts_df\": \"sku_id\",\n",
    "    \"fact_transactions_df\": \"order_id\",\n",
    "    \"hier_clnd_df\": \"fscldt_id\",\n",
    "    \"hier_hldy_df\": \"hldy_id\",\n",
    "    \"hier_invloc_df\": \"loc\",\n",
    "    \"hier_invstatus_df\": \"code_id\",\n",
    "    \"hier_possite_df\": \"site_id\",\n",
    "    \"hier_pricestate_df\": \"substate_id\",\n",
    "    \"hier_prod_df\": \"sku_id\",\n",
    "    \"hier_rtlloc_df\": \"str\"\n",
    "}\n",
    "\n",
    "for df_name, primary_key in dataframes.items():\n",
    "    df = globals()[df_name]\n",
    "    \n",
    "    # Non-null check for primary key\n",
    "    null_count = df.filter(col(primary_key).isNull()).count()\n",
    "    print(f\"Null values in '{primary_key}' column of {df_name}: {null_count}\")  # Should be 0\n",
    "\n",
    "    # Uniqueness check for primary key\n",
    "    distinct_count = df.select(countDistinct(primary_key)).collect()[0][0]\n",
    "    total_rows = df.count()\n",
    "    print(f\"Is '{primary_key}' unique in {df_name}? {distinct_count == total_rows}\")  # Should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e94e3c-cc08-4bb0-bcad-fff64734ffca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing the string part from the column code_id\n",
    "hier_invstatus_df = hier_invstatus_df.withColumn(\"code_id\", regexp_extract(\"code_id\", r'\\d+', 0).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c63e925-026a-4026-a78c-1e25c7a7a947",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Foreign Key check"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid foreign keys in 'sku_id': 0\nInvalid foreign keys in 'fscldt_id': 2206380\nInvalid foreign keys in 'substate_id': 0\nInvalid foreign keys in 'code_id': 13886\nInvalid foreign keys in 'sku_id': 0\n"
     ]
    }
   ],
   "source": [
    "invalid_fk_count1 = fact_transactions_df.join(hier_prod_df, fact_transactions_df[\"sku_id\"] == hier_prod_df[\"sku_id\"], \"left_anti\").count()\n",
    "print(f\"Invalid foreign keys in 'sku_id': {invalid_fk_count1}\")  # Should be 0\n",
    "\n",
    "invalid_fk_count2 = fact_transactions_df.join(hier_clnd_df, fact_transactions_df[\"fscldt_id\"] == hier_clnd_df[\"fscldt_id\"], \"left_anti\").count()\n",
    "print(f\"Invalid foreign keys in 'fscldt_id': {invalid_fk_count2}\") \n",
    "\n",
    "invalid_fk_count3 = fact_transactions_df.join(hier_pricestate_df, fact_transactions_df[\"price_substate_id\"] == hier_pricestate_df[\"substate_id\"], \"left_anti\").count()\n",
    "print(f\"Invalid foreign keys in 'substate_id': {invalid_fk_count3}\") \n",
    "\n",
    "invalid_fk_count4 = fact_transactions_df.join(hier_invstatus_df, fact_transactions_df[\"line_id\"] == hier_invstatus_df[\"code_id\"], \"left_anti\").count()\n",
    "print(f\"Invalid foreign keys in 'code_id': {invalid_fk_count4}\") \n",
    "\n",
    "invalid_fk_count5 = fact_transactions_df.join(fact_averagecosts_df, fact_transactions_df[\"sku_id\"] == fact_averagecosts_df[\"sku_id\"], \"left_anti\").count()\n",
    "print(f\"Invalid foreign keys in 'sku_id': {invalid_fk_count5}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9f739bb-fbc4-4b40-95e6-165aa636adc6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleaning the table"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e20333e-a9f3-464c-adc7-92ea883e9f4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalize Hierarchy Tables"
    }
   },
   "outputs": [],
   "source": [
    "# hier_prod_df\n",
    "# Create separate tables for each level\n",
    "hier_prod_sku_id_df = hier_prod_df.select(\"sku_id\", \"sku_label\").distinct()\n",
    "hier_clnd_df = hier_clnd_df.select(\"fscldt_id\", \"fscldt_label\").distinct()\n",
    "\n",
    "hier_pricestate = hier_pricestate_df.select(\"substate_id\", \"substate_label\").distinct()\n",
    "hier_invstatus = hier_invstatus_df.select(\"code_id\", \"code_label\").distinct()\n",
    "\n",
    "# Save to staging schema\n",
    "hier_prod_sku_id_df.write.mode(\"overwrite\").saveAsTable(\"Energy.hier_prod_sku_id_df\")\n",
    "hier_clnd_df.write.mode(\"overwrite\").saveAsTable(\"Energy.hier_clnd_df\")\n",
    "hier_pricestate.write.mode(\"overwrite\").saveAsTable(\"Energy.hier_pricestate_df\")\n",
    "hier_invstatus.write.mode(\"overwrite\").saveAsTable(\"Energy.hier_invstatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d961b1f-8d42-4ba9-8238-14a539880d2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Refined Table"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "# Create a refined table called mview_weekly_sales which totals sales_units,\n",
    "# sales_dollars, and discount_dollars by pos_site_id, sku_id, fsclwk_id,\n",
    "# price_substate_id and type.\n",
    "# Aggregate sales data\n",
    "mview_weekly_sales_df = fact_transactions_df.groupBy(\n",
    "    \"pos_site_id\",\n",
    "    \"sku_id\",\n",
    "    \"fscldt_id\",\n",
    "    \"price_substate_id\",\n",
    "    \"type\"\n",
    ").agg(\n",
    "    sum(\"sales_units\").alias(\"total_sales_units\"),\n",
    "    sum(\"sales_dollars\").alias(\"total_sales_dollars\"),\n",
    "    sum(\"discount_dollars\").alias(\"total_discount_dollars\")\n",
    ")\n",
    "\n",
    "# Save to refined schema\n",
    "mview_weekly_sales_df.write.mode(\"overwrite\").saveAsTable(\"Energy.mview_weekly_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6abf83d5-e807-4d0a-af6f-4acbb96828be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "write transformation logic that will incrementally calculate all the\n",
    "totals in the above table for partially loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4603aec0-cb3c-4895-a471-86dd2937f7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "%python\n",
    "# Assume 'load_timestamp' is a column in the fact table\n",
    "new_data_df = fact_transactions_df.filter(col(\"dt\") > \"2023-10-01\")\n",
    "\n",
    "# Aggregate new data\n",
    "new_aggregates_df = new_data_df.groupBy(\n",
    "    \"pos_site_id\",\n",
    "    \"sku_id\",\n",
    "    \"fscldt_id\",\n",
    "    \"price_substate_id\",\n",
    "    \"type\"\n",
    ").agg(\n",
    "    sum(\"sales_units\").alias(\"total_sales_units\"),\n",
    "    sum(\"sales_dollars\").alias(\"total_sales_dollars\"),\n",
    "    sum(\"discount_dollars\").alias(\"total_discount_dollars\")\n",
    ")\n",
    "\n",
    "# Save the existing table as a Delta table if not already done\n",
    "mview_weekly_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/dev/Energy_DE/mview_weekly_sales\")\n",
    "\n",
    "# Load the Delta table\n",
    "# mview_weekly_sales_delta = DeltaTable.forPath(spark, \"/path/to/mview_weekly_sales\")\n",
    "\n",
    "# Merge new aggregates into existing Delta table\n",
    "new_aggregates_df.createOrReplaceTempView(\"new_aggregates\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO delta.`/mnt/dev/Energy_DE/mview_weekly_sales` AS target\n",
    "USING new_aggregates AS source\n",
    "ON target.pos_site_id = source.pos_site_id\n",
    "   AND target.sku_id = source.sku_id\n",
    "   AND target.fscldt_id = source.fscldt_id\n",
    "   AND target.price_substate_id = source.price_substate_id\n",
    "   AND target.type = source.type\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "        total_sales_units = target.total_sales_units + source.total_sales_units,\n",
    "        total_sales_dollars = target.total_sales_dollars + source.total_sales_dollars,\n",
    "        total_discount_dollars = target.total_discount_dollars + source.total_discount_dollars\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (pos_site_id, sku_id, fscldt_id, price_substate_id, type, total_sales_units, total_sales_dollars, total_discount_dollars)\n",
    "    VALUES (source.pos_site_id, source.sku_id, source.fscldt_id, source.price_substate_id, source.type, source.total_sales_units, source.total_sales_dollars, source.total_discount_dollars)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7363013229481811,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "DE01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
